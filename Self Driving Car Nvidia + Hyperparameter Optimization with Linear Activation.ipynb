{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import pi\n",
    "from itertools import islice\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Preprocessing\n"
     ]
    }
   ],
   "source": [
    "#reading the data set from data.txt\n",
    "\n",
    "dataset_path='H:\\Case Study\\Self Driving Case Study\\Autopilot-TensorFlow-master\\Autopilot-TensorFlow-master\\driving_dataset'\n",
    "training_path=os.path.join(dataset_path,'data.txt')\n",
    "\n",
    "split=0.8\n",
    "X=[]\n",
    "y=[]\n",
    "\n",
    "#islice(text, start, stop, LIMIT) --> ('ABCDEF', 2, None) --> 'C D E F'  -->https://docs.python.org/2/library/itertools.html\n",
    "with open(training_path) as f:\n",
    "    for line in islice(f,None):\n",
    "        path,angle=line.strip().split()\n",
    "        full_path=os.path.join(dataset_path,path)\n",
    "        X.append(full_path)\n",
    "        y.append((float(angle)*pi)/180)\n",
    "        \n",
    "y=np.array(y)\n",
    "\n",
    "print(\"Finished Preprocessing\")\n",
    "\n",
    "#split it with 80/20\n",
    "\n",
    "split_index=int(len(y)*0.8)\n",
    "y_train=y[:split_index]\n",
    "y_val=y[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Reading from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train', 'rb') as f:\n",
    "    X_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_val', 'rb') as f:\n",
    "    X_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36324, 66, 200, 3)\n",
      "(9082, 66, 200, 3)\n",
      "(36324,)\n",
      "(9082,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 200 3\n"
     ]
    }
   ],
   "source": [
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "channels = X_train.shape[3]\n",
    "\n",
    "print(img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], channels, img_rows, img_cols).astype('float32')\n",
    "    X_val = X_val.reshape(X_val.shape[0], channels, img_rows, img_cols).astype('float32')\n",
    "    img_size = (channels, img_rows, img_cols)\n",
    "    \n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, channels).astype('float32')\n",
    "    X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, channels).astype('float32')\n",
    "    img_size = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255.0\n",
    "X_val = X_val/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36324, 66, 200, 3)\n",
      "(9082, 66, 200, 3)\n",
      "(66, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building model from Nvidia Architecture with Hyperparameter Optimization(Using regularization and Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def custom_activation(x):\n",
    "    return tf.multiply(tf.math.atan(x), 2) #  arctan range is [-pi/2, pi/2]. so we have to get the angle between [-pi, pi] so we are multiplying by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to refer the model architecture: https://developer.nvidia.com/blog/deep-learning-self-driving-cars/\n",
    "#to use atan as a activation function refer: https://www.tensorflow.org/api_docs/python/tf/math/atan\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "img_input = tf.keras.Input(img_size)\n",
    "x = tf.keras.layers.Conv2D(filters=24, kernel_size=(5,5), strides=(2,2), padding='valid',activation='relu',name='conv-1')(img_input)\n",
    "x = tf.keras.layers.Conv2D(filters=36, kernel_size=(5,5), strides=(2,2), padding='valid',activation='relu', name='conv-2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Conv2D(filters=48, kernel_size=(5,5), strides=(2,2), padding='valid',activation='relu',kernel_regularizer =tf.keras.regularizers.l1( l=0.01), name='conv-3')(x)\n",
    "x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='valid',activation='relu',kernel_regularizer =tf.keras.regularizers.l1( l=0.01), name='conv-4')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='valid',activation='relu',kernel_regularizer =tf.keras.regularizers.l1( l=0.01), name='conv-5')(x)\n",
    "x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "x = tf.keras.layers.Dense(1164, activation='relu', name='dense_layer-1')(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name='dropout-1')(x)\n",
    "x = tf.keras.layers.Dense(100, activation='relu', name='dense_layer-2')(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name='dropout-2')(x)\n",
    "x = tf.keras.layers.Dense(50, activation='relu', name='dense_layer-3')(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name='dropout-3')(x)\n",
    "x = tf.keras.layers.Dense(10, activation='relu', name='dense_layer-4')(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name='dropout-4')(x)\n",
    "output = tf.keras.layers.Dense(1, activation=custom_activation, name='output')(x) #note we have used atan to use tan inverse activation\n",
    "\n",
    "model_2 = tf.keras.models.Model(inputs=img_input, outputs=output, name='model-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compiling and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "Epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
    "\n",
    "checkpoint_file_name = 'H:\\Case Study\\Self Driving Case Study\\Checkpoints\\envideahyper\\envidiahyper-weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_file_name, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36324 samples, validate on 9082 samples\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 11.56089, saving model to H:\\Case Study\\Self Driving Case Study\\Checkpoints\\envideahyper\\envidiahyper-weights.01-11.56.hdf5\n",
      "36324/36324 - 262s - loss: 23.0090 - val_loss: 11.5609\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 11.56089 to 1.38812, saving model to H:\\Case Study\\Self Driving Case Study\\Checkpoints\\envideahyper\\envidiahyper-weights.02-1.39.hdf5\n",
      "36324/36324 - 247s - loss: 5.3965 - val_loss: 1.3881\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.38812 to 0.63725, saving model to H:\\Case Study\\Self Driving Case Study\\Checkpoints\\envideahyper\\envidiahyper-weights.03-0.64.hdf5\n",
      "36324/36324 - 253s - loss: 0.6456 - val_loss: 0.6373\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63725 to 0.26436, saving model to H:\\Case Study\\Self Driving Case Study\\Checkpoints\\envideahyper\\envidiahyper-weights.04-0.26.hdf5\n",
      "36324/36324 - 254s - loss: 0.3290 - val_loss: 0.2644\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26436\n",
      "36324/36324 - 256s - loss: 0.2726 - val_loss: 0.2705\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26436 to 0.25776, saving model to H:\\Case Study\\Self Driving Case Study\\Checkpoints\\envideahyper\\envidiahyper-weights.06-0.26.hdf5\n",
      "36324/36324 - 254s - loss: 0.2376 - val_loss: 0.2578\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25776\n",
      "36324/36324 - 248s - loss: 0.2082 - val_loss: 0.4074\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25776\n",
      "36324/36324 - 259s - loss: 0.1908 - val_loss: 0.4310\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25776\n",
      "36324/36324 - 275s - loss: 0.1717 - val_loss: 0.3123\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25776\n",
      "36324/36324 - 253s - loss: 0.1609 - val_loss: 0.4741\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.25776\n",
      "36324/36324 - 274s - loss: 0.1520 - val_loss: 0.5581\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25776\n",
      "36324/36324 - 252s - loss: 0.1421 - val_loss: 0.5008\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.25776\n",
      "36324/36324 - 249s - loss: 0.1381 - val_loss: 0.3794\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.25776\n",
      "36324/36324 - 258s - loss: 0.1289 - val_loss: 0.4101\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.25776\n",
      "36324/36324 - 253s - loss: 0.1231 - val_loss: 0.3595\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.25776\n",
      "36324/36324 - 250s - loss: 0.1215 - val_loss: 0.3219\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.25776\n",
      "36324/36324 - 245s - loss: 0.1166 - val_loss: 0.4303\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.25776\n",
      "36324/36324 - 242s - loss: 0.1136 - val_loss: 0.5949\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.25776\n",
      "36324/36324 - 243s - loss: 0.1107 - val_loss: 0.4180\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.25776\n",
      "36324/36324 - 242s - loss: 0.1037 - val_loss: 0.4031\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.25776\n",
      "36324/36324 - 249s - loss: 0.1023 - val_loss: 0.4757\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.25776\n",
      "36324/36324 - 240s - loss: 0.1198 - val_loss: 0.3938\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.25776\n",
      "36324/36324 - 239s - loss: 0.0983 - val_loss: 0.2830\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.25776\n",
      "36324/36324 - 239s - loss: 0.1074 - val_loss: 0.6186\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.25776\n",
      "36324/36324 - 238s - loss: 0.0937 - val_loss: 0.6030\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.25776\n",
      "36324/36324 - 258s - loss: 0.0940 - val_loss: 0.4819\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.25776 to 0.24781, saving model to H:\\Case Study\\Self Driving Case Study\\Checkpoints\\envideahyper\\envidiahyper-weights.27-0.25.hdf5\n",
      "36324/36324 - 299s - loss: 0.0887 - val_loss: 0.2478\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.24781\n",
      "36324/36324 - 269s - loss: 0.0873 - val_loss: 0.5103\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.24781 to 0.23717, saving model to H:\\Case Study\\Self Driving Case Study\\Checkpoints\\envideahyper\\envidiahyper-weights.29-0.24.hdf5\n",
      "36324/36324 - 287s - loss: 0.1039 - val_loss: 0.2372\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.23717\n",
      "36324/36324 - 316s - loss: 0.0915 - val_loss: 0.3820\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_2.fit(X_train, y_train, batch_size=batch_size, epochs=Epochs, validation_data=(X_val, y_val), callbacks=callbacks, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
